{"posts":[{"title":"STM32 入门篇(一)","content":"Bit-Banding [位带操作] STM32是32位架构的处理器，因此它的寄存器是32位的，处理器也是对于32位的数据进行操作，我们将32位称为一个字(word)。 STM32寄存器可以存储四个字节的数据。因此对于位置寄存器而言，我们可以使用一个8位的16进制数来表达地址数值。由于处理器是32位的，因此STM32可以一次访问连续四个字节上的数据。例如我们索引存储在某个区域（例如SRAM）的寄存器数据时，一般会写为0x20000000.0 - 0x20000000.31, 下一个字的数据（由另一个寄存器存储）则是从0x2000004.0开始。其中每个比特位bit的数据可以被置为0或1。 同时我们可以知道一个32单片机的内存容量最大是232bytes2^32 bytes232bytes数据，每个地址(0x20000000.0- 0x20000000.7)为8bit的数据，作为最小的数据单元。 但是，这样对于某个bit位置上的数据进行读写涉及到对于这一串寄存器四个字节的数据的读取，与或非操作与再次存储，导致对于某一个比特位的数据改变不是很方便快捷。因此我们为了实现原子操作，会使用位带操作，将原始位带1bit的数据位映射为位带别名区一个寄存器32bit（4字节/1字）的数据位。在位带别名区通过平移字的单位并且取这个字的LSB(Least Significance Bit 最低有效位)，我们可以更快获取，修改位带区一个寄存器中32bit [四个字节] 中特定一位的数据。 因此，我们可以非常方便的写出地址映射关系，即： bit_band_address = alias_region_base + (region_base_offset ×\\times× 32) + (bit_number ×\\times× 4) 或者是 bit_band_address = alias_region_base + (region_base_offset ×\\times× 8 + bit_number) ×\\times× 4 其中，888 说明一个地址有8比特， 444说明每个比特映射为4个字节。 ","link":"https://cybding.github.io/Dblog/post/stm32-intro1/"},{"title":"What is ESTimators? (Applied in MLR)","content":"参考资料: https://www.stat.purdue.edu/~fmliang/STAT553/lect6.pdf ","link":"https://cybding.github.io/Dblog/post/what-is-estimators-applied-in-mlr/"},{"title":"坐标系与旋转矩阵 ","content":"引言 一段时间接触机器人相关的运动学控制问题，对于空间中坐标变换经常犯错，本文将对一些常用的变换做一个总结，梳理对于空间坐标的描述关系。 坐标系 坐标系，即一组由正交基底构成的，可以表示任意一个空间坐标并表示方法唯一。常见的坐标系为固定坐标系(Fixed Frame) 与 随动坐标系 (Body Frame)。我们将用大写字母 A,B,⋯A, B, \\cdotsA,B,⋯来表示这些坐标系。 很多时候机器人的姿态并不与地面的坐标系保持一致，这时我们所需要将在地面坐标系中的向量(Eg. 距离向量，速度向量或加速度向量) 转换到机器人的随动坐标系中，这时就要我们用到两种比较类似的工具 旋转矩阵(Rotational Matrix) 与 旋转算符(Rotational Operator)。实际上两者都是一个变换矩阵，但是由于意义有一定差异，在名称上沿用CS223A课程的习惯，进行一定的区分。 空间点的表达(Configuration) 旋转矩阵 对于一个空间中任意位置的坐标系，我们需要描述它的状态需要四个量：原点的位置与这个坐标系相对于原坐标系下的基底坐标，即APB,AXB,AYB,AZB^AP_B, ^AX_B, ^AY_B, ^AZ_BAPB​,AXB​,AYB​,AZB​。为什么我们需要新坐标系相对于原坐标系AAA下的坐标呢，因为这样我们就可以通过新坐标系下的向量去分别线性组合对应基底在AAA下的坐标从而获取空间点在AAA下的坐标。 这样我们可以写出在坐标系B下任意位置的空间点在A下的坐标为 AX→=APB+BARB X→=APB+[AXB,AYB,AZB]BX→^A\\overrightarrow{X} = ^AP_B +{}_B^AR{}^B~\\overrightarrow{X} = ^AP_B + [^AX_B, ^AY_B, ^AZ_B]{}^B\\overrightarrow{X}AX=APB​+BA​RB X=APB​+[AXB​,AYB​,AZB​]BX。由于坐标系变换的对称性，我们可以写出 BAR=ABRT_B^AR = _A^BR^\\mathrm{T}BA​R=AB​RT。 我们称这样的变换矩阵BAR_B^ARBA​R，由将一个坐标系BBB下的向量变换至另一个坐标系AAA，称为旋转矩阵。这个过程也被成为映射(Mapping), 将空间中一个点从一个坐标系框架映射到另一个坐标系框架。 Homogeneous Transformation 旋转算符 与旋转矩阵不同，旋转算符不是对于同一个点的坐标系间表达的映射，而是在同一个坐标系下，将一个点旋转至另一个点(Notice: 在fixed Rotation中，这个方法可以帮助我们获取我们新坐标系在固定坐标系下的坐标表达，从而构建出上文所讲到的旋转矩阵)。 从上图可以看到，旋转算符围绕着XXX轴，在垂直于XXX轴的Y,ZY,ZY,Z平面上进行旋转。θ\\thetaθ是按照右手螺旋定则(大拇指指向固定轴正方向)所确定的正方向，即图中逆时针旋转角度。我们记绕着XXX轴旋转θ\\thetaθ角度的旋转算符为(A^AA)RXR_XRX​(θ\\thetaθ)。 这样，我们实际上可以把先前的 Homogeneous Transformation分解为两个部分，首先进行Translation，将坐标原点进行变换；其次对于在BBB原点处进行旋转(由于我们一般希望能够使用更加简洁的旋转角度构建在此处的旋转矩阵，即计算BAR(θ)_B^AR(\\theta)BA​R(θ))。一般而言，我们有两种方式构建这个矩阵，一种是Fixed Angle, 一种采取Euler Angle，两者结果相同，殊途同归。 ","link":"https://cybding.github.io/Dblog/post/Coordinate-and-RotationalMatrices/"},{"title":"PCA vs FA, 我们应该如何选择？","content":"引言 从了解到数据处理常用的两种降维方法以来，我一直没有处理清楚这两种方法：主成分分析(PCA)与因子分析(FA)的关系，导致每次思考使用这两种方法时心中存在疑惑与不解。在查阅一些网络资料后，本文将对这两种方法进行详细阐述，并在文章最后，总结一下了解到的主要区别。 PCA ① 模型 假设我们有一个数据矩阵Xm×nX_{m\\times n}Xm×n​, 矩阵的第ttt列代表着第ttt次次观测得到的数据，而矩阵的每一行则为一个随机变量进行独立的KKK次观测所得到的K-dimension随机向量。因此，我们可以将矩阵XXX写为[X1,X2,⋯ ,Xm][X_1, X_2,\\cdots, X_m][X1​,X2​,⋯,Xm​]. 对于一个 mmm较大的矩阵而言，每次观测得到的数据信息太多，以至于无法准确描述每次观测较为重要的特征，因此我们希望对于数据提取关键特征进行降维处理。 那什么叫提取关键特征呢？实际上，也就是在对于单个mmm维的观测数据而言，我们选择一个合适的基底，使得每次观测的结果在这些方向上能体现出较大的差异(最大化单个方向的方差)，从而能够将数据简化为对于这些具有较大区分度维度上的投影(有时候也会记这个投影的数值为数据的PCA Score，形成一个新的指标)，并尽可能保留最多的信息。 ② 数学基础 协方差 对于一个随机变量向量 XXX, 根据方差的定义，有Var(Xi)=Cov(Xi,Xi)=E[Xi⋅Xi]−E[Xi]2Var(X^i) = Cov(X^i, X^i) = E[X^i\\cdot X^i] - E[X^i]^2Var(Xi)=Cov(Xi,Xi)=E[Xi⋅Xi]−E[Xi]2与Cov(Xi,Xj)=E[Xi⋅Xj]−E[Xi]⋅E[Xj]Cov(X^i, X^j) = E[X^i\\cdot X^j] - E[X^i]\\cdot E[X^j]Cov(Xi,Xj)=E[Xi⋅Xj]−E[Xi]⋅E[Xj]。 我们定义协方差矩阵为Σ(i,j)=Cov(Xi,Xj)\\Sigma_(i, j) = Cov(X^i, X^j)Σ(​i,j)=Cov(Xi,Xj)CovX=E[X⋅XT]−E[X]⋅E[X]TCov_X = E[X\\cdot X^T] - E[X]\\cdot E[X]^TCovX​=E[X⋅XT]−E[X]⋅E[X]T, 整理得，Σ=E[(X−E[X])⋅(X−E[X])T]\\Sigma = E[(X - E[X])\\cdot (X - E[X])^T]Σ=E[(X−E[X])⋅(X−E[X])T]。 通常，我们需要通过观测结果矩阵来估计，而无法直接获取到协方差矩阵。对于任意的一个观测矩阵Xm×nX_{m\\times n}Xm×n​, 我们可以估计 E[Xi]=1nΣj=1nXji,(i=1,2,⋯ ,m)E[X^i] = \\frac{1}{n} \\Sigma_{j=1}^n X_j^i, (i = 1, 2, \\cdots, m)E[Xi]=n1​Σj=1n​Xji​,(i=1,2,⋯,m)。同时，根据无偏估计，E[Xi⋅Xj]=1n−1Σk=1nXki⋅XkjE[X_i\\cdot X_j] = \\frac{1}{n-1} \\Sigma_{k=1}^n X_k^i \\cdot X_k^jE[Xi​⋅Xj​]=n−11​Σk=1n​Xki​⋅Xkj​. 因此，我们可以使用1n−1(X−μX⋅1T)(X−μX⋅1T)T\\dfrac{1}{n-1}(X-\\mu_X \\cdot\\mathrm{1}^T)(X-\\mu_X\\cdot\\mathrm{1}^T)^Tn−11​(X−μX​⋅1T)(X−μX​⋅1T)T来估计协方差矩阵SSS。 特征分解 对于一个实对称矩阵而言，我们常用的性质是： 它具有实特征值。(Sx)∗Sx=x∗S2x=λ2x∗x=λ2∣∣x∣∣2≥0(Sx)^*Sx = x^*S^2x = \\lambda^2x^*x = \\lambda^2||x||^2 \\geq 0(Sx)∗Sx=x∗S2x=λ2x∗x=λ2∣∣x∣∣2≥0, 由于要求 λ2∈R,λ2≥0\\lambda^2 \\in \\mathrm{R}, \\lambda^2 \\geq 0λ2∈R,λ2≥0，则得λ\\lambdaλ为实数。 它可以被分解为 Q\\SigmaQ^T, 其中QQQ是正交矩阵。 对于一个协方差矩阵而言，由于其可以写为S=XXTS = XX^TS=XXT, 因此对于任何一个向量vvv, 总有vTXXTv≥0v^TXX^Tv \\geq 0vTXXTv≥0。原矩阵是一个半正定矩阵(Semi-Positive Matrix)，因此特征值均大于等于000。 ② 数学推导 首先我们对于一些模型求解的目标进行明确： 我们希望找到一组基底，使得对于矩阵Xm×nX_{m\\times n}Xm×n​进行变换后，在这些方向上，能够获得最大的方差，并将变换后的矩阵记为X′X^{&#x27;}X′。(Maximize Var(X′i)Var(X^{&#x27;i})Var(X′i)) 同时我们希望这组基底，使得在这个基底下，每次观测得到的mmm维坐标是独立的。对于原始矩阵XXX而言，随机变量间可能存在协方差Σ(i,j)≠0\\Sigma(i, j)\\neq 0Σ(i,j)​=0。对于我们变换后的矩阵，X′X^{&#x27;}X′, 它的协方差矩阵 Σ′=diag(λ1,λ2,⋯ ,λm)\\Sigma^{&#x27;} = diag(\\lambda_1, \\lambda_2, \\cdots, \\lambda_m)Σ′=diag(λ1​,λ2​,⋯,λm​), 即没有变量间的相关性，我们找到的特征是独立的。 我们首先考虑找到第一个方向u1u_1u1​(∣∣u1∣∣2=1||u_1||_2 = 1∣∣u1​∣∣2​=1), 则在这个方向上的投影长度可以记为u1TXu_1^TXu1T​X,在这个方向上的方差可以记为u1TXXTu1=u1TSu1u_1^TXX^Tu_1 = u_1^TSu_1u1T​XXTu1​=u1T​Su1​(预处理将数据矩阵减去μX1T\\mu_X\\mathrm{1}^TμX​1T，则E[u1TX]=u1TE[X]=u1T0=0E[u_1^TX] = u_1^TE[X] = u_1^T\\mathrm{0} = 0E[u1T​X]=u1T​E[X]=u1T​0=0). 这个方差描述了数据在这个方向上的分散情况，若u1TSu1=0u_1^TSu_1 = 0u1T​Su1​=0，则说明数据矩阵的所有向量$$均垂直于这个方向，在这个方向没有方差。 此时我们的目标简化为，u1=argmax(u1TSu1)u_1 = argmax(u_1^TSu_1)u1​=argmax(u1T​Su1​)。由于协方差矩阵是对称矩阵，根据谱分解(Spectrum Theory), 我们可以将矩阵SSS分解为Σi=1mλiqiqiT(qiTqj=δij)\\Sigma_{i=1}^m \\lambda_i q_iq_i^T (q_i^Tq_j = \\delta_{ij})Σi=1m​λi​qi​qiT​(qiT​qj​=δij​)。不难证明，我们将u1u_1u1​选择为特征值λi\\lambda_iλi​最大的那个方向所对应的特征向量qiq_iqi​，表达式将获得最大值。[这个结论也可以通过矩阵求导得到，读者可以自行尝试] 类似的，我们可以将其他基底的选择简化为在与u1,u2,⋯ ,uku_1, u_2, \\cdots, u_ku1​,u2​,⋯,uk​垂直，∣∣uk+1∣∣2=1||u_{k+1}||_2 = 1∣∣uk+1​∣∣2​=1的情况下，最大化uk+1TSuk+1u_{k+1}^TSu_{k+1}uk+1T​Suk+1​, 可以证明，即对应特征分解的qk+1q_{k+1}qk+1​向量。这样，我们就找到了所希望获得的基底。 ③ 算法描述 输入数据，nnn个独立观测的mmm维数据，X1,X2,⋯ ,XnX_1, X_2, \\cdots, X_nX1​,X2​,⋯,Xn​,构成数据矩阵Xm×nX_{m\\times n}Xm×n​ 计算数据矩阵估计得到的Empirical covariance matrix SSS（经验协方差矩阵） 计算对应的特征分解 S=UDUTS = UDU^TS=UDUT，其中D=diag(λ1,λ2,⋯ ,λm),lambda1≥λ2≥⋯≥λmD = diag(\\lambda_1, \\lambda_2, \\cdots, \\lambda_m), lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_mD=diag(λ1​,λ2​,⋯,λm​),lambda1​≥λ2​≥⋯≥λm​, UUU是一个正交矩阵。 选择 p&lt;mp&lt;mp&lt;m, 并且将基底组 Vm×pV_{m\\times p}Vm×p​ 中的元素设置为对应的正交矩阵前ppp个元素(q1,q2,⋯ ,qnq_1, q_2, \\cdots, q_nq1​,q2​,⋯,qn​)。 则在新基底下的每次测量结果可以表示为Yp×n=VTXY_{p\\times n} = V^TXYp×n​=VTX。 ④ 模型评估 对于我们已经选取的方向进行的数据降维，我们怎么评判数据降维的效果呢？ 一般而言使用方差解释比率评估。由于在进行基底变换后，每个方向上的方差是独立的，因此对于一个mmm维的数据而言，它的总方差可以表示为 λ1+λ2+⋯+λm\\lambda_1 + \\lambda_2 +\\cdots + \\lambda_mλ1​+λ2​+⋯+λm​。【实际上，即使原来的协方差矩阵不是对角矩阵，但是每个维度定义在一个常规的直角坐标系下，因此可以认为总方差就是单个维度方差的累加，这时协方差不影响数据总方差的大小(λ1+λ2+⋯+λm=tr(AAT)\\lambda_1 + \\lambda_2 +\\cdots + \\lambda_m = tr(AA^T)λ1​+λ2​+⋯+λm​=tr(AAT))，我们实际忽略了协方差在相互作用的影响，而是寻求找到最大方差的方向】 而由于我们只选取了ppp个维度进行数据投影，则降维数据YYT=VTXXTV=VSVT=UT[,1:p]SU[,1:p]=diag(λ1,lambda2,⋯ ,λp)YY^T = V^TXX^TV = VSV^T = U^T[, 1:p]SU[, 1:p] = diag(\\lambda_1, lambda_2, \\cdots, \\lambda_p)YYT=VTXXTV=VSVT=UT[,1:p]SU[,1:p]=diag(λ1​,lambda2​,⋯,λp​)，解释的方差为 λ1+λ2+⋯+λp\\lambda_1 + \\lambda_2 +\\cdots + \\lambda_pλ1​+λ2​+⋯+λp​。定义Ve=λ1+λ2+⋯+λpλ1+λ2+⋯+λm=1−αV_e = \\dfrac{\\lambda_1 + \\lambda_2 +\\cdots + \\lambda_p}{\\lambda_1 + \\lambda_2 +\\cdots + \\lambda_m} = 1 - \\alphaVe​=λ1​+λ2​+⋯+λm​λ1​+λ2​+⋯+λp​​=1−α。α\\alphaα是没有解释的方差比率，VeV_eVe​是解释的方差比率，这个值是小于等于111的非负数。当我们取p=mp = mp=m时，我们没有进行降维，因此Ve=1V_e = 1Ve​=1。 PCA vs. FA https://stats.stackexchange.com/a/288646 ","link":"https://cybding.github.io/Dblog/post/pca-vs-fa/"}]}